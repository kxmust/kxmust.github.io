<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Ken</title><link>https://kxmust.github.io</link><description>乐观是一种生活态度，保持学习和记录。</description><copyright>Ken</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/99197662?v=4</url><title>avatar</title><link>https://kxmust.github.io</link></image><lastBuildDate>Mon, 21 Apr 2025 12:23:55 +0000</lastBuildDate><managingEditor>Ken</managingEditor><ttl>60</ttl><webMaster>Ken</webMaster><item><title>深度学习2-Softmax回归(常见的损失函数，Fashion-MNIST)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-2-Softmax-hui-gui-%28-chang-jian-de-sun-shi-han-shu-%EF%BC%8CFashion-MNIST%29.html</link><description># Softmax回归

softmax回归其实是一个分类问题。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-2-Softmax-hui-gui-%28-chang-jian-de-sun-shi-han-shu-%EF%BC%8CFashion-MNIST%29.html</guid><pubDate>Mon, 21 Apr 2025 12:23:26 +0000</pubDate></item><item><title>深度学习1-线性回归</title><link>https://kxmust.github.io/post/shen-du-xue-xi-1--xian-xing-hui-gui.html</link><description># 线性回归
利用一个简单的线性回归的例子来了解神经网络的实现逻辑。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-1--xian-xing-hui-gui.html</guid><pubDate>Fri, 18 Apr 2025 09:15:23 +0000</pubDate></item></channel></rss>