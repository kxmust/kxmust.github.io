<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Ken</title><link>https://kxmust.github.io</link><description>乐观是一种生活态度，保持学习和记录。</description><copyright>Ken</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/99197662?v=4</url><title>avatar</title><link>https://kxmust.github.io</link></image><lastBuildDate>Sun, 27 Apr 2025 05:32:50 +0000</lastBuildDate><managingEditor>Ken</managingEditor><ttl>60</ttl><webMaster>Ken</webMaster><item><title>深度学习9-AlexNet深度卷积神经网络</title><link>https://kxmust.github.io/post/shen-du-xue-xi-9-AlexNet-shen-du-juan-ji-shen-jing-wang-luo.html</link><description># AlexNet深度卷积神经网络
AlexNet是引起了深度学习热潮的第一个网络

&gt; 在深度学习之前，最常用的机器学习方法是核方法
首先提取特征
利用核函数来计算相关性，判断高维空间内两个点是否有相关性
经过核函数处理之后就会变成凸优化问题
有非常好的定理和数学基础

现在SVM支持向量机也被广泛使用，因为它不怎么需要调参，对参数不怎么敏感

在早些年，计算机视觉的工作主要在特征提取方面
如果将原始图像直接输入SVM效果还非常差
因此，需要科学家或者工程师都提出了大量的方法来抽取图片中的特征信息

AlexNet 赢得了2012年的ImageNet竞赛

## 1 AlexNet模型

主要改进：
Dropout  (模型变大了，用dropout来正则)
ReLU
MaxPooling

AlexNet就是一个更深更大的LeNet网络，两个网络结果对比如下：

![Image](https://github.com/user-attachments/assets/feb4ae15-401d-4cec-9484-b2e6dd84d3f7)


相比于LeNet，AlexNet使用了更大的卷积核，更大的步长，因为输入的图片更大，并且使用了更大的池化窗口，使用了MaxPooling而不是AvgPooling
并且增加了更多的卷积层
最后也用了三层全连接层

更多细节：
激活函数从Sigmoid变为了ReLU，减缓梯度消失
隐藏全连接层后加入了Dropout层
做了数据增强(将图片做了很多变化，随机截取，调节亮度，随机调节色温来增加数据的变种)

AlexNet的参数量大概是46M，LeNet大概有0.6M
AlexNet做一次先前计算大概比LeNet贵了250倍

总结：
AlexNet是一个更深的LeNet，75X的参数个数，250X的计算复杂度
新引入了丢弃法(Dropout)，ReLU，最大池化层，和数据增强
AlexNet赢下了2012年的ImageNet竞赛，标志着新一轮的神经网络热潮的开始

## 2 AlexNet的代码实现

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    # 这里使用一个11*11的更大窗口来捕捉对象。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-9-AlexNet-shen-du-juan-ji-shen-jing-wang-luo.html</guid><pubDate>Sun, 27 Apr 2025 05:27:14 +0000</pubDate></item><item><title>深度学习8-CNN(LeNet模型)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-8-CNN%28LeNet-mo-xing-%29.html</link><description># LeNet模型
LeNet是最早比较经典的CNN模型，用来识别邮件上的邮编，或者支票中的金额。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-8-CNN%28LeNet-mo-xing-%29.html</guid><pubDate>Sat, 26 Apr 2025 12:55:56 +0000</pubDate></item><item><title>深度学习7-CNN基础</title><link>https://kxmust.github.io/post/shen-du-xue-xi-7-CNN-ji-chu.html</link><description># CNN卷积神经网络
## 1 CNN基础
从多层感知机（MLP）到卷积神经网络（CNN）的演进中，​​平移不变性​​和​​局部性​​是两大核心设计原则，它们解决了传统MLP处理图像时的低效问题，并成为卷积操作的理论基础。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-7-CNN-ji-chu.html</guid><pubDate>Fri, 25 Apr 2025 14:19:12 +0000</pubDate></item><item><title>深度学习6-Pytorch GPU的使用</title><link>https://kxmust.github.io/post/shen-du-xue-xi-6-Pytorch%20GPU-de-shi-yong.html</link><description># GPU的使用

使用!nvidia-smi来查看GPU状态
```python
!nvidia-smi
```

```python
import torch
from torch import nn
torch.device('cpu'), torch.cuda.device('cuda') # torch.cuda.device('cuda:1') 访问第一个GPU

```

定义两个函数来测试是否存在GPU如果没有则用CPU

```python
def try_gpu(i=0): #@save
    '''如果存在，则返回gpu(i)，否则返回cpu()'''
    if torch.cuda.device_count() &gt;= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')
    
def try_all_gpus(): #@save
    '''返回所有可用的GPU，如果没有GPU，则返回[cpu(),]'''
    devices = [torch.device(f'cuda:{i}')
        for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]
    
try_gpu(), try_gpu(10), try_all_gpus()
```

查询张量坐在的设备

```python
x = torch.tensor([1, 2, 3])
x.device
```
创建张量时，放在GPU上

```python
# 创建张量时，放在GPU上
X = torch.ones(2, 3, device=try_gpu())
X

# 也可以在第二个GPU上创建张量
# Y = torch.rand(2, 3, device=try_gpu(1))
```

如果你有多个GPU，可以将一个GPU上的值copy到另一个GPU上
Z = X.cuda(1)  可以将X的值从GPU0 copy到GPU1
数值在一个GPU上才能做计算

在GPU上做神经网络

```python
# 在GPU上做神经网络
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())  # net.to将神经网络挪动到0号GPU上 ,等于将网络的参数在0号GPU上copy一份

net(X)  # x也在0号GPU上
net[0].weight.data.device  # 查看权重参数所在的位置
```




。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-6-Pytorch%20GPU-de-shi-yong.html</guid><pubDate>Thu, 24 Apr 2025 09:15:14 +0000</pubDate></item><item><title>深度学习5-Pytorch(模型搭建, 权重初始化, 权重保存和加载)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-5-Pytorch%28-mo-xing-da-jian-%2C%20-quan-zhong-chu-shi-hua-%2C%20-quan-zhong-bao-cun-he-jia-zai-%29.html</link><description># Pytorch基础
## 1 Pytorch基础知识

用nn.Sequential模组搭建一个简单的神经网络
```python
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

X = torch.rand(2, 20)

net(X)

```
nn.Sequential是一个模组，我们也可以自己定义一个模组来构建神经网络

```python

class MLP(nn.Module):   # 表示是nn模组的子类
    def __init__(self):   # 定义有哪些参数
        super().__init__()  # 调用一个父类，来先设置需要的内部的参数
        self.hidden = nn.Linear(20, 256)   # 定义一个线下层存在一个成员变量中
        self.out = nn.Linear(256,10)

    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))

net = MLP()
net(X)

```

如何实现前面的nn.Sequential模组
```python

class MySequential(nn.Module):
    def __init__(self, *args):  #*args表示接入一个有序列表, **表示字典
        super().__init__()
        for block in args:
            self._modules[block] = block  # 是一个有序的字典,_modules是一个特殊的容器，表示放进去的是每一层网络

    def forward(self, X):
        for block in self._modules.values():
            X = block(X)    #利用每一层网络处理输入X
        return X

net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)

```

当Sequential这个类无法满足我们的计算需求时,我们可以自定义类来实现特殊的计算

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-5-Pytorch%28-mo-xing-da-jian-%2C%20-quan-zhong-chu-shi-hua-%2C%20-quan-zhong-bao-cun-he-jia-zai-%29.html</guid><pubDate>Thu, 24 Apr 2025 08:47:28 +0000</pubDate></item><item><title>深度学习4-数值稳定性优化(权重初始化和激活函数的选择)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-4--shu-zhi-wen-ding-xing-you-hua-%28-quan-zhong-chu-shi-hua-he-ji-huo-han-shu-de-xuan-ze-%29.html</link><description># 数值稳定性
在模型训练过程中，计算梯度是可以理解为多个矩阵的乘法，当网络模型很深时，如果每个矩阵中的值都大于0，这样就会让梯度变得非常大，导致梯度爆炸，这对模型的训练来说是致命的。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-4--shu-zhi-wen-ding-xing-you-hua-%28-quan-zhong-chu-shi-hua-he-ji-huo-han-shu-de-xuan-ze-%29.html</guid><pubDate>Wed, 23 Apr 2025 09:26:29 +0000</pubDate></item><item><title>深度学习3-多层感知机</title><link>https://kxmust.github.io/post/shen-du-xue-xi-3--duo-ceng-gan-zhi-ji.html</link><description># 多层感知机
## 1 感知机
1. 感知机是一个二分类模型，输出0或者1(或者-1,1)，是最早的AI模型之一。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-3--duo-ceng-gan-zhi-ji.html</guid><pubDate>Tue, 22 Apr 2025 08:04:43 +0000</pubDate></item><item><title>深度学习2-Softmax回归(常见的损失函数，Fashion-MNIST)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-2-Softmax-hui-gui-%28-chang-jian-de-sun-shi-han-shu-%EF%BC%8CFashion-MNIST%29.html</link><description># Softmax回归

softmax回归其实是一个分类问题。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-2-Softmax-hui-gui-%28-chang-jian-de-sun-shi-han-shu-%EF%BC%8CFashion-MNIST%29.html</guid><pubDate>Mon, 21 Apr 2025 12:23:26 +0000</pubDate></item><item><title>深度学习1-线性回归</title><link>https://kxmust.github.io/post/shen-du-xue-xi-1--xian-xing-hui-gui.html</link><description># 线性回归
利用一个简单的线性回归的例子来了解神经网络的实现逻辑。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-1--xian-xing-hui-gui.html</guid><pubDate>Fri, 18 Apr 2025 09:15:23 +0000</pubDate></item></channel></rss>