<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Ken</title><link>https://kxmust.github.io</link><description>乐观是一种生活态度，保持学习和记录。</description><copyright>Ken</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/99197662?v=4</url><title>avatar</title><link>https://kxmust.github.io</link></image><lastBuildDate>Tue, 06 May 2025 08:57:29 +0000</lastBuildDate><managingEditor>Ken</managingEditor><ttl>60</ttl><webMaster>Ken</webMaster><item><title>深度学习19-目标检测(锚框)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-19--mu-biao-jian-ce-%28-mao-kuang-%29.html</link><description># 目标检测（锚框）
目标检测需要给出物体的边缘框
一类目标检测算法是基于锚框（anchor box）的：
- 提出多个被称为锚框的区域（边缘框），是对物体位置的猜测
- 预测每个锚框里是否含有关注的物体
- 如果这个锚框中还有关注的物体，则预测从这个锚框到真实边缘框的偏移

## 1 锚框的生成

### 1.1 计算两个框之间的相似度（IoU交并比）

![Image](https://github.com/user-attachments/assets/d644019c-5775-4bbd-afaa-481849aa46b8)

比如给定两个集合A和B，计算IoU过程如下：

![Image](https://github.com/user-attachments/assets/734fd2aa-c3fd-4fc0-a755-e43349a6d636)

对于图片来说，两个锚框的相似度就是重叠部分的像素值

### 1.2 赋予锚框标号
- 每个锚框是一个训练样本
- 将每个锚框要么标注为背景，要么关联上一个真实的边缘框，计算其偏移用于训练
- 我们可能会生成大量的锚框，这样会导致大量的负类样本

比如生成了9个锚框，识别的物体个数为4，标注过程如下

![Image](https://github.com/user-attachments/assets/834389c9-ccb6-431f-b199-99c0931afff5)

- 对每个锚框计算与每个物体边缘框的IoU，会生成一个矩阵，如上图所示
- 选出矩阵中最大值，比如是x23，它对应边缘框3，那么将该锚框用于预测边缘框3，然后删除它所在行和所在列的所有数据
- 对于剩下的数据，继续选择最大值，然后关联上边缘框，继续下去，直到将所有边缘框都关联上了锚框

值得注意的是：
- 每读取一张图片，都要生成一次锚框，比如9个锚框，然后对其进行标号，生成训练样本
- 赋予标号的算法有多种，上面只是其中常见的方法

### 1.3 使用非极大值抑制(NMS)输出
因为每个锚框会预测一个边缘框，其中有很多相似的预测，可以通过NMS来合并相似的预测
- 选中是非背景类的最大预测值(最大预测值是指对类的预测的softmax值)
- 去掉所有其他锚框和该锚框IoU值大于\theta的预测
- 重复上述过程，直到所有预测框要么被选中，要么被去掉


总结：
- 一类目标检测算法基于锚框来预测
- 首先生成大量锚框，并赋予标号，每个锚框作为一个样本进行训练
- 在预测时，使用NMS来取消冗余的预测

## 2 代码实现

### 2.1 锚框的生成和显示

锚框的生成（其中一种方法）
以每一个像素为中心，生成不同高宽度的锚框

![Image](https://github.com/user-attachments/assets/6b63a0a7-7883-41e9-9f9c-5f36e5a2897e)
- s 表示锚框大小，占图片的百分之多少
- r 锚框的高宽比
- 每次生成的时候会给出多个s和多个r，但是不会一一组合，因为会生成大量的锚框，通常一般做法是用一个最合适的s和每个r进行一次组合，然后将后续的s与r1进行组合
- 所以生成的锚框数量=s的个数+r的个数-1


```python
%matplotlib inline
import torch
from d2l import torch as d2l
torch.set_printoptions(2) # 精简输出精度

#@save
# data 图片, size 锚框的大小，占图片的大小, ratio表示锚框的高宽比
def multibox_prior(data, sizes, ratios):
    '''生成以每个像素为中心具有不同形状的锚框'''
    in_height, in_width = data.shape[-2:]
    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)
    boxes_per_pixel = (num_sizes + num_ratios - 1)
    size_tensor = torch.tensor(sizes, device=device)
    ratio_tensor = torch.tensor(ratios, device=device)
    
    # 为了将锚点移动到像素的中心，需要设置偏移量。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-19--mu-biao-jian-ce-%28-mao-kuang-%29.html</guid><pubDate>Tue, 06 May 2025 08:56:55 +0000</pubDate></item><item><title>深度学习18-目标检测(数据集)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-18--mu-biao-jian-ce-%28-shu-ju-ji-%29.html</link><description># 目标检测
识别一张图片中的所有物体，比如多个狗或者猫，并且还要用方框标注出每个物体的位置

**一个边缘框可以用四个数字来定义：**
左上x，右上y， 右下x和右下y   （注意，一个一个图片的左上角为原点）
左上x，右上y，宽和高

**目标检测数据集：**
一个图片中可能有多个类，所以一般用CSV文件来存
一行表示一个物体
所以一张图片可能需要多行来描述
每一行的数据包括：
图片名文件，物体类别和边缘框

**常用的目标检测数据集：**
COCO数据集 ：cocodataset.org  包含了80个常见类别，包含了大概330k图片，有1.5M物体

## 1 边缘框的实现

读入一张图片
```python
%matplotlib inline

import torch
from d2l import torch as d2l

d2l.set_figsize()
img = d2l.plt.imread('../Jupyter/img/catdog.jpg')
d2l.plt.imshow(img);
```

![Image](https://github.com/user-attachments/assets/1c5be667-5caa-4f8d-a72c-840b7fab2622)

框坐标的转换：
```python
# 定义在这两种表示之间进行转换的函数
def box_corner_to_center(boxes):
    '''从（左上，右下）转换到（中间，宽度，高度）'''
    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    cx = (x1 + x2) / 2  # 得到中间点的坐标
    cy = (y1 + y2) / 2

    w = x2 - x1
    h = y2 - y1

    boxes = torch.stack((cx, cy, w, h), axis = -1)
    return boxes

def box_center_to_corner(boxes):
    '''从（中间，宽度，高度）转换到（左上，右下）'''
    # 图片左上角为零点,向下是y轴正方向
    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h 

    boxes = torch.stack((x1, y1, x2, y2), axis = -1)
    return boxes
```
基于边缘框画出物体的位置：

```python
# bbox是边界框的英文缩写
dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]
boxes = torch.tensor((dog_bbox, cat_bbox))
box_center_to_corner(box_corner_to_center(boxes)) == boxes   # 测试一下转换函数

def bbox_to_rect(bbox, color):
    return d2l.plt.Rectangle(xy = (bbox[0], bbox[1]),
                            width = bbox[2]-bbox[0],
                            height = bbox[3] - bbox[1],
                            edgecolor = color,
                            linewidth = 2,
                            fill = False)

fig = d2l.plt.imshow(img)
fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))
fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));
```

![Image](https://github.com/user-attachments/assets/5f242be6-b3b6-4fac-bf2c-b66f24d3cb6f)

## 2 目标检测数据集(手动构造了一个小的数据集)

下载和读取数据集

```python
#包含所有图像和CSV标签文件的香蕉检测数据集可以直接从互联网下载。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-18--mu-biao-jian-ce-%28-shu-ju-ji-%29.html</guid><pubDate>Mon, 05 May 2025 09:24:11 +0000</pubDate></item><item><title>深度学习17-迁移学习(微调)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-17--qian-yi-xue-xi-%28-wei-diao-%29.html</link><description># 迁移学习(微调)
如果我们想要设计一个模型来做某个任务，而已经有类似的现有模型了，这时候我们可以对现有模型进行微调来完成新的任务

## 微调 Fine-tuning
我们可以将一个神经网络模型的分为两个部分，输出层用于对物品进行分类，其他层用来提取特征

那么对于一个类似的任务，我们可以将已经训练好的模型的前N层拿过来，然后对最后的输出层进行修改，然后利用我们收集的数据进行微调，就能很好的完成新的任务。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-17--qian-yi-xue-xi-%28-wei-diao-%29.html</guid><pubDate>Tue, 29 Apr 2025 13:25:58 +0000</pubDate></item><item><title>深度学习16-数据增广</title><link>https://kxmust.github.io/post/shen-du-xue-xi-16--shu-ju-zeng-guang.html</link><description># 数据增广

图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模

随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力

例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-16--shu-ju-zeng-guang.html</guid><pubDate>Tue, 29 Apr 2025 11:34:01 +0000</pubDate></item><item><title>深度学习15-多GPU并行</title><link>https://kxmust.github.io/post/shen-du-xue-xi-15--duo-GPU-bing-xing.html</link><description># 单机多卡并行
一个机器可以安装多个GPU
在训练和预测时，可以将小批量计算切分到多个GPU上来达到加速的目的

常用的切分方案有：
- 数据并行-将小批量数据分成n块，每个GPU拿到完整的参数计算一块数据的梯度（通常性能更好）
- 模型并行-（将模型分成n块，每个GPU拿到一块模型计算它的前向和反向结果，通常适用于模型大到单GPU放不下）
- 通道并行（数据+模型并行）

## 1 数据并行

以两个GPU为例：
- 在任何一次迭代中，给定的随机小批量数据将被分成两份，然后均匀分配给每个GPU
- 每个GPU会利用分配的数据计算梯度
- 每个GPU会将计算的梯度发出去，并且进行相加，以获得当前小批量的随机梯度
- 将计算的聚合梯度发送给每个GPU，每个GPU利用得到的随机梯度来更新模型参数

大致的流程图为：

![Image](https://github.com/user-attachments/assets/b10e95f5-9f1d-4768-a684-f6103fa8f600)

总结：
-  当一个模型能用到单卡计算时，通常使用数据并行拓展到多卡上
- 模型并行则用在超大模型上

## 2 数据并行的从0实现

以LeNet为例来来实现

- 先构建LeNet模型

```python
%matplotlib inline
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

# 初始化模型参数
scale = 0.01
W1 = torch.randn(size=(20, 1, 3, 3)) * scale
b1 = torch.zeros(20)
W2 = torch.randn(size=(50, 20, 5, 5)) * scale
b2 = torch.zeros(50)
W3 = torch.randn(size=(800, 128)) * scale
b3 = torch.zeros(128)
W4 = torch.randn(size=(128, 10)) * scale
b4 = torch.zeros(10)
params = [W1, b1, W2, b2, W3, b3, W4, b4]


# 定义模型
def lenet(X, params):
    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])
    h1_activation = F.relu(h1_conv)
    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))
    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])
    h2_activation = F.relu(h2_conv)
    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))
    h2 = h2.reshape(h2.shape[0],-1)
    h3_linear = torch.mm(h2, params[4]) + params[5]
    h3 = F.relu(h3_linear)
    y_hat = torch.mm(h3, params[6]) + params[7]
    return y_hat
    
# 交叉熵损失函数
loss = nn.CrossEntropyLoss(reduction='none')
```

- 向多个设备分发参数

```python
# 给一个参数，然后将它发送到哪个GPU上
def get_params(params, device):  
    new_params = [p.to(device) for p in params] 
    for p in new_params:    # 需要计算梯度
        p.requires_grad_()
    return new_params

# 将所有参数复制到一个GPU
new_params = get_params(params, d2l.try_gpu(0))
print('b1 权重:', new_params[1])
print('b1 梯度:', new_params[1].grad)
```

- 使用allreduce函数将所有向量相加，并将结果广播给所有GPU

```python
# 将所有向量相加,并广播给所有GPU
def allreduce(data):
    for i in range(1, len(data)):
        data[0][:] += data[i].to(data[0].device)  # 将数据复制到GPU0上进行相加
    for i in range(1, len(data)):
        data[i][:] = data[0].to(data[i].device)  # GPU0计算得到的结果发送给所有GPU

data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]
print('allreduce之前：\n', data[0], '\n', data[1])
allreduce(data)
print('allreduce之后：\n', data[0], '\n', data[1])
```

- 将一个小批量数据均匀分布在多个GPU上

```python
data = torch.arange(20).reshape(4, 5)
devices = [torch.device('cuda:0'), torch.device('cuda:1')]
split = nn.parallel.scatter(data, devices)  # 将数据均匀切开，分配给每个GPU上
print('input :', data)
print('load into', devices)
print('output:', split)

```

- 为了方便使用，定义一个split_batch函数，将数据和标签都进行拆分

```python
def split_batch(X, y, devices):
    '''将X和y拆分到多个设备上'''
    assert X.shape[0] == y.shape[0]
    return (nn.parallel.scatter(X, devices),
            nn.parallel.scatter(y, devices))
```

- 定义一个多GPU训练小批量数据的函数

```python
# 定义一个多GPU训练小批量数据的函数
def train_batch(X, y, device_params, devices, lr): # 输入,所有GPU上的参数，GPU,学习率
    X_shards, y_shards = split_batch(X, y, devices)  # 拆分数据
    # 在每个GPU上分别计算损失, ls中包含了所有GPU中的损失
    ls = [loss(lenet(X_shard, device_W), y_shard).sum()
                    for X_shard, y_shard, device_W in zip(
                        X_shards, y_shards, device_params)]
    for l in ls: # 反向传播在每个GPU上分别执行
        l.backward()
    # 将每个GPU的所有梯度相加，并将其广播到所有GPU
    with torch.no_grad():
        for i in range(len(device_params[0])):
            allreduce([device_params[c][i].grad for c in range(len(devices))])
    # 在每个GPU上分别更新模型参数
    for param in device_params:
        d2l.sgd(param, lr, X.shape[0]) # 在这里，我们使用全尺寸的小批量
```

- 开始训练

```python
def train(num_gpus, batch_size, lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
    devices = [d2l.try_gpu(i) for i in range(num_gpus)]
    # 将模型参数复制到num_gpus个GPU
    device_params = [get_params(params, d) for d in devices]
    num_epochs = 10
    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])
    timer = d2l.Timer()
    for epoch in range(num_epochs):
        timer.start()
        for X, y in train_iter:
        # 为单个小批量执行多GPU训练
            train_batch(X, y, device_params, devices, lr)
            torch.cuda.synchronize()  # 同步一次,保证每个GPU都完成了
        timer.stop()
        # 在GPU0上评估模型
        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(
            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))
    print(f'测试精度：{animator.Y[0][-1]:.2f}，{timer.avg():.1f}秒/轮，'
            f'在{str(devices)}')

# 单GPU运行
train(num_gpus=1, batch_size=256, lr=0.2)

# 多GPU运行
train(num_gpus=2, batch_size=256, lr=0.2)
```

[代码链接](https://github.com/kxmust/Deep_learning_note/blob/main/18.1parallel_computing.ipynb)

## 3 数据并行的简单实现

- 首先构建模型

```python
import torch
from torch import nn
from d2l import torch as d2l

# 定义模型
def resnet18(num_classes, in_channels=1):
    '''稍加修改的ResNet-18模型'''
    def resnet_block(in_channels, out_channels, num_residuals,
                    first_block=False):
        blk = []
        for i in range(num_residuals):
            if i == 0 and not first_block:
                blk.append(d2l.Residual(out_channels,
                        use_1x1conv=True, strides=2))
            else:
                blk.append(d2l.Residual(out_channels, out_channels))
        return nn.Sequential(*blk)

    # 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层
    net = nn.Sequential(
        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm2d(64),
        nn.ReLU())
    net.add_module('resnet_block1', resnet_block(64, 64, 2, first_block=True))
    net.add_module('resnet_block2', resnet_block(64, 128, 2))
    net.add_module('resnet_block3', resnet_block(128, 256, 2))
    net.add_module('resnet_block4', resnet_block(256, 512, 2))
    net.add_module('global_avg_pool', nn.AdaptiveAvgPool2d((1,1)))
    net.add_module('fc', nn.Sequential(nn.Flatten(),
    nn.Linear(512, num_classes)))
    return net
```

- 网络初始化

```python
# 网络初始化
net = resnet18(10)
# 获取GPU列表
devices = d2l.try_all_gpus()
# 我们将在训练代码实现中初始化网络
```

- 训练函数

```python
def train(net, num_gpus, batch_size, lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
    devices = [d2l.try_gpu(i) for i in range(num_gpus)]
    
    def init_weights(m):
        if type(m) in [nn.Linear, nn.Conv2d]:
            nn.init.normal_(m.weight, std=0.01)
    net.apply(init_weights)
    
    # 在多个GPU上设置模型
    net = nn.DataParallel(net, device_ids=devices) #给定一个网络,给定GPU，然后将Net复制到每个GPU上
    trainer = torch.optim.SGD(net.parameters(), lr)
    loss = nn.CrossEntropyLoss()
    timer, num_epochs = d2l.Timer(), 10
    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])
    for epoch in range(num_epochs):
        net.train()
        timer.start()
        for X, y in train_iter:
            trainer.zero_grad()
            X, y = X.to(devices[0]), y.to(devices[0])
            l = loss(net(X), y)
            l.backward()
            trainer.step()
        timer.stop()
        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))
    print(f'测试精度：{animator.Y[0][-1]:.2f}，{timer.avg():.1f}秒/轮，'
        f'在{str(devices)}')

# 单卡训练
train(net, num_gpus=1, batch_size=256, lr=0.1)
# 多卡训练
train(net, num_gpus=2, batch_size=512, lr=0.2)
```

[代码链接](https://github.com/kxmust/Deep_learning_note/blob/main/18.2parallel_computing_simple.ipynb)
。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-15--duo-GPU-bing-xing.html</guid><pubDate>Tue, 29 Apr 2025 08:26:50 +0000</pubDate></item><item><title>深度学习14-ResNet残差网络</title><link>https://kxmust.github.io/post/shen-du-xue-xi-14-ResNet-can-cha-wang-luo.html</link><description># ResNet残差网络

考虑这样一个问题，网络是不是做的越深越好
网络越深越能拟合更加复杂的问题？
但是随着网络的加深，模型训练过程中可能会发生偏差，可能性能不会增加，而开销会增大

我们要解决的问题是：当增加新的层的时候至少不会让模型变差。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-14-ResNet-can-cha-wang-luo.html</guid><pubDate>Mon, 28 Apr 2025 10:08:51 +0000</pubDate></item><item><title>深度学习13-Batch_Normalization</title><link>https://kxmust.github.io/post/shen-du-xue-xi-13-Batch_Normalization.html</link><description># Batch Normalization 批量归一化
考虑一个问题，当一个网络很深的情况下
梯度反传过程中，上面的梯度会更大，下面的梯度会更小
这样会导致上面的网络会很快收敛，而下面的网络会更新很慢

这样会有一个什么问题呢？
当下面的参数发生变化后，对输入抽象出来的特征会发生变化，则上面的网络又需要重新训练
导致收敛变慢

因此我们可以用batch normalization 来将输出的均值和方差进行固定，
这样的好处是在训练底部时可以避免变化顶部层

## 1 批量归一化

对于一个批量数据，计算均值和方差

![Image](https://github.com/user-attachments/assets/ae2e4981-d004-4362-ba02-249d5e4b4f6a)

利用均值和方差来进行归一化

![Image](https://github.com/user-attachments/assets/d56f9e07-9069-4606-9060-715ed691bee4)

特点：
可以学习的参数是\beta 和 \gamma
作用的位置：
 全连接层和卷积层的输出上，激活函数前
 全连接层和卷积层的输入上
对于全连接层，作用在特征维
对于卷积层，作用在通道层

总结：
批量归一化固定小批量的均值和方差，然后学习出适合的偏移的缩放
可以加速收敛速度，但一般不改变模型精度

## 批量归一化实现

从0实现批量归一化

```python
import torch
from torch import nn
from d2l import torch as d2l

# 批量归一化从0开始实现,（输入，（可以学的两个参数），全局的均值，全局的方差，避免除0，用来更新全局均值和方差的）
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
  # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式
  if not torch.is_grad_enabled():
    # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
    X_hat = (X- moving_mean) / torch.sqrt(moving_var + eps)
  else:
    assert len(X.shape) in (2, 4)
    if len(X.shape) == 2:
      # 使用全连接层的情况，计算特征维上的均值和方差
      mean = X.mean(dim=0)
      var = ((X- mean) ** 2).mean(dim=0)
    else:
      # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-13-Batch_Normalization.html</guid><pubDate>Mon, 28 Apr 2025 08:52:05 +0000</pubDate></item><item><title>深度学习12-GoogLeNet(Inception块)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-12-GoogLeNet%28Inception-kuai-%29.html</link><description># GoogLeNet模型

它是一种含并行连接的网络
之前的模型比如LeNet，AlexNet，V2G用了不同规模的卷积神经网络来提取图片特征，哪到底选择哪种？

GoogLeNet的做法就是全都要

## 1 GoogLeNet模型的结构

### 1.1 Inception块
Inception块从四个不同的路径来抽取不同层面的信息，然后在输出通道维度进行合并

![Image](https://github.com/user-attachments/assets/110d6acd-620b-4d01-a1ae-22881c317bd7)

Inception块由四条并行路径组成

前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，
从不同空间大小中提取信息。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-12-GoogLeNet%28Inception-kuai-%29.html</guid><pubDate>Sun, 27 Apr 2025 13:10:00 +0000</pubDate></item><item><title>深度学习11-NiN模型(利用1x1的卷积层来取代全连接神经网络)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-11-NiN-mo-xing-%28-li-yong-1x1-de-juan-ji-ceng-lai-qu-dai-quan-lian-jie-shen-jing-wang-luo-%29.html</link><description># NiN卷积神经网络模型
无论是LeNet, AlexNet或者VGG都是使用了一系列卷积层拉提取空间结构特征，然后使用全连接层对特征的表征进行处理。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-11-NiN-mo-xing-%28-li-yong-1x1-de-juan-ji-ceng-lai-qu-dai-quan-lian-jie-shen-jing-wang-luo-%29.html</guid><pubDate>Sun, 27 Apr 2025 09:14:19 +0000</pubDate></item><item><title>深度学习10-VGG(更深更大的CNN模型)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-10-VGG%28-geng-shen-geng-da-de-CNN-mo-xing-%29.html</link><description># VGG模型
AlexNet比LeNet更深更大，得到更好的精度
能不能设计的更深更大？

有哪些选项：
更多全连接层（太贵）
更多的卷积层
将卷积层组合成块

## 1 VGG的结构
VGG块将多个卷积层和一个池化层组合成块：

![Image](https://github.com/user-attachments/assets/d1f806b7-8baa-4021-9eab-d59295263eeb)

它这里面用了3x3的卷积核，原因是在相同的计算开销下，3x3比5
x5的卷积核拥有更好的效果。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-10-VGG%28-geng-shen-geng-da-de-CNN-mo-xing-%29.html</guid><pubDate>Sun, 27 Apr 2025 08:04:34 +0000</pubDate></item><item><title>深度学习9-AlexNet深度卷积神经网络</title><link>https://kxmust.github.io/post/shen-du-xue-xi-9-AlexNet-shen-du-juan-ji-shen-jing-wang-luo.html</link><description># AlexNet深度卷积神经网络
AlexNet是引起了深度学习热潮的第一个网络

&gt; 在深度学习之前，最常用的机器学习方法是核方法
首先提取特征
利用核函数来计算相关性，判断高维空间内两个点是否有相关性
经过核函数处理之后就会变成凸优化问题
有非常好的定理和数学基础

现在SVM支持向量机也被广泛使用，因为它不怎么需要调参，对参数不怎么敏感

在早些年，计算机视觉的工作主要在特征提取方面
如果将原始图像直接输入SVM效果还非常差
因此，需要科学家或者工程师都提出了大量的方法来抽取图片中的特征信息

AlexNet 赢得了2012年的ImageNet竞赛

## 1 AlexNet模型

主要改进：
Dropout  (模型变大了，用dropout来正则)
ReLU
MaxPooling

AlexNet就是一个更深更大的LeNet网络，两个网络结果对比如下：

![Image](https://github.com/user-attachments/assets/feb4ae15-401d-4cec-9484-b2e6dd84d3f7)


相比于LeNet，AlexNet使用了更大的卷积核，更大的步长，因为输入的图片更大，并且使用了更大的池化窗口，使用了MaxPooling而不是AvgPooling
并且增加了更多的卷积层
最后也用了三层全连接层

更多细节：
激活函数从Sigmoid变为了ReLU，减缓梯度消失
隐藏全连接层后加入了Dropout层
做了数据增强(将图片做了很多变化，随机截取，调节亮度，随机调节色温来增加数据的变种)

AlexNet的参数量大概是46M，LeNet大概有0.6M
AlexNet做一次先前计算大概比LeNet贵了250倍

总结：
AlexNet是一个更深的LeNet，75X的参数个数，250X的计算复杂度
新引入了丢弃法(Dropout)，ReLU，最大池化层，和数据增强
AlexNet赢下了2012年的ImageNet竞赛，标志着新一轮的神经网络热潮的开始

## 2 AlexNet的代码实现

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    # 这里使用一个11*11的更大窗口来捕捉对象。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-9-AlexNet-shen-du-juan-ji-shen-jing-wang-luo.html</guid><pubDate>Sun, 27 Apr 2025 05:27:14 +0000</pubDate></item><item><title>深度学习8-CNN(LeNet模型)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-8-CNN%28LeNet-mo-xing-%29.html</link><description># LeNet模型
LeNet是最早比较经典的CNN模型，用来识别邮件上的邮编，或者支票中的金额。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-8-CNN%28LeNet-mo-xing-%29.html</guid><pubDate>Sat, 26 Apr 2025 12:55:56 +0000</pubDate></item><item><title>深度学习7-CNN基础</title><link>https://kxmust.github.io/post/shen-du-xue-xi-7-CNN-ji-chu.html</link><description># CNN卷积神经网络
## 1 CNN基础
从多层感知机（MLP）到卷积神经网络（CNN）的演进中，​​平移不变性​​和​​局部性​​是两大核心设计原则，它们解决了传统MLP处理图像时的低效问题，并成为卷积操作的理论基础。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-7-CNN-ji-chu.html</guid><pubDate>Fri, 25 Apr 2025 14:19:12 +0000</pubDate></item><item><title>深度学习6-Pytorch GPU的使用</title><link>https://kxmust.github.io/post/shen-du-xue-xi-6-Pytorch%20GPU-de-shi-yong.html</link><description># GPU的使用

使用!nvidia-smi来查看GPU状态
```python
!nvidia-smi
```

```python
import torch
from torch import nn
torch.device('cpu'), torch.cuda.device('cuda') # torch.cuda.device('cuda:1') 访问第一个GPU

```

定义两个函数来测试是否存在GPU如果没有则用CPU

```python
def try_gpu(i=0): #@save
    '''如果存在，则返回gpu(i)，否则返回cpu()'''
    if torch.cuda.device_count() &gt;= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')
    
def try_all_gpus(): #@save
    '''返回所有可用的GPU，如果没有GPU，则返回[cpu(),]'''
    devices = [torch.device(f'cuda:{i}')
        for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]
    
try_gpu(), try_gpu(10), try_all_gpus()
```

查询张量坐在的设备

```python
x = torch.tensor([1, 2, 3])
x.device
```
创建张量时，放在GPU上

```python
# 创建张量时，放在GPU上
X = torch.ones(2, 3, device=try_gpu())
X

# 也可以在第二个GPU上创建张量
# Y = torch.rand(2, 3, device=try_gpu(1))
```

如果你有多个GPU，可以将一个GPU上的值copy到另一个GPU上
Z = X.cuda(1)  可以将X的值从GPU0 copy到GPU1
数值在一个GPU上才能做计算

在GPU上做神经网络

```python
# 在GPU上做神经网络
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())  # net.to将神经网络挪动到0号GPU上 ,等于将网络的参数在0号GPU上copy一份

net(X)  # x也在0号GPU上
net[0].weight.data.device  # 查看权重参数所在的位置
```




。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-6-Pytorch%20GPU-de-shi-yong.html</guid><pubDate>Thu, 24 Apr 2025 09:15:14 +0000</pubDate></item><item><title>深度学习5-Pytorch(模型搭建, 权重初始化, 权重保存和加载)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-5-Pytorch%28-mo-xing-da-jian-%2C%20-quan-zhong-chu-shi-hua-%2C%20-quan-zhong-bao-cun-he-jia-zai-%29.html</link><description># Pytorch基础
## 1 Pytorch基础知识

用nn.Sequential模组搭建一个简单的神经网络
```python
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

X = torch.rand(2, 20)

net(X)

```
nn.Sequential是一个模组，我们也可以自己定义一个模组来构建神经网络

```python

class MLP(nn.Module):   # 表示是nn模组的子类
    def __init__(self):   # 定义有哪些参数
        super().__init__()  # 调用一个父类，来先设置需要的内部的参数
        self.hidden = nn.Linear(20, 256)   # 定义一个线下层存在一个成员变量中
        self.out = nn.Linear(256,10)

    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))

net = MLP()
net(X)

```

如何实现前面的nn.Sequential模组
```python

class MySequential(nn.Module):
    def __init__(self, *args):  #*args表示接入一个有序列表, **表示字典
        super().__init__()
        for block in args:
            self._modules[block] = block  # 是一个有序的字典,_modules是一个特殊的容器，表示放进去的是每一层网络

    def forward(self, X):
        for block in self._modules.values():
            X = block(X)    #利用每一层网络处理输入X
        return X

net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)

```

当Sequential这个类无法满足我们的计算需求时,我们可以自定义类来实现特殊的计算

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-5-Pytorch%28-mo-xing-da-jian-%2C%20-quan-zhong-chu-shi-hua-%2C%20-quan-zhong-bao-cun-he-jia-zai-%29.html</guid><pubDate>Thu, 24 Apr 2025 08:47:28 +0000</pubDate></item><item><title>深度学习4-数值稳定性优化(权重初始化和激活函数的选择)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-4--shu-zhi-wen-ding-xing-you-hua-%28-quan-zhong-chu-shi-hua-he-ji-huo-han-shu-de-xuan-ze-%29.html</link><description># 数值稳定性
在模型训练过程中，计算梯度是可以理解为多个矩阵的乘法，当网络模型很深时，如果每个矩阵中的值都大于0，这样就会让梯度变得非常大，导致梯度爆炸，这对模型的训练来说是致命的。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-4--shu-zhi-wen-ding-xing-you-hua-%28-quan-zhong-chu-shi-hua-he-ji-huo-han-shu-de-xuan-ze-%29.html</guid><pubDate>Wed, 23 Apr 2025 09:26:29 +0000</pubDate></item><item><title>深度学习3-多层感知机</title><link>https://kxmust.github.io/post/shen-du-xue-xi-3--duo-ceng-gan-zhi-ji.html</link><description># 多层感知机
## 1 感知机
1. 感知机是一个二分类模型，输出0或者1(或者-1,1)，是最早的AI模型之一。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-3--duo-ceng-gan-zhi-ji.html</guid><pubDate>Tue, 22 Apr 2025 08:04:43 +0000</pubDate></item><item><title>深度学习2-Softmax回归(常见的损失函数，Fashion-MNIST)</title><link>https://kxmust.github.io/post/shen-du-xue-xi-2-Softmax-hui-gui-%28-chang-jian-de-sun-shi-han-shu-%EF%BC%8CFashion-MNIST%29.html</link><description># Softmax回归

softmax回归其实是一个分类问题。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-2-Softmax-hui-gui-%28-chang-jian-de-sun-shi-han-shu-%EF%BC%8CFashion-MNIST%29.html</guid><pubDate>Mon, 21 Apr 2025 12:23:26 +0000</pubDate></item><item><title>深度学习1-线性回归</title><link>https://kxmust.github.io/post/shen-du-xue-xi-1--xian-xing-hui-gui.html</link><description># 线性回归
利用一个简单的线性回归的例子来了解神经网络的实现逻辑。</description><guid isPermaLink="true">https://kxmust.github.io/post/shen-du-xue-xi-1--xian-xing-hui-gui.html</guid><pubDate>Fri, 18 Apr 2025 09:15:23 +0000</pubDate></item></channel></rss>