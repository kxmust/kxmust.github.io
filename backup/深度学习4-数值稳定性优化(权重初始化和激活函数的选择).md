# 数值稳定性
在模型训练过程中，计算梯度是可以理解为多个矩阵的乘法，当网络模型很深时，如果每个矩阵中的值都大于0，这样就会让梯度变得非常大，导致梯度爆炸，这对模型的训练来说是致命的。当矩阵中的数值很小时，就会让梯度趋近于0，导致消失，这也会极大影响模型的训练。

为了解决这个问题，我们希望每层的输出和梯度的均值和方差都为一个常数，比如均值为0，方差为\gamma_t。

## 1 权重的初始化
假设权重w中的每一个元素都是独立同分布的，并且均值为0，方差为\gamma_t。
每一层的输出独立于权重w
不考虑激活函数的情况下
这样的设定下，每一层的输出的均值为0.


![Image](https://github.com/user-attachments/assets/c1da1866-ff84-4eed-ba35-13f0c29cdc68)

方差的计算为：

![Image](https://github.com/user-attachments/assets/eee84726-3be0-49ab-a117-3173918dfd93)

反向计算过程也是类似的：

![Image](https://github.com/user-attachments/assets/69d6a196-27a2-410a-917a-d77684347628)

这样的设定下均值都为0，但是方差要为常数的话需要满足两个条件：
n_{t-1}*\gamma_t = 1
n_t*\gamma_t = 1

但是这两个条件无法同时满足，因此Xavier初始化方法做了一个取舍：

![Image](https://github.com/user-attachments/assets/2d7e280e-4849-4c74-84a6-85ef7491c6bb)

## 2 激活函数的选择
引入激活函数后，如果需要让输出和梯度的均值和方差都为常数的话，激活函数必须为：
F(x)=x

我们将常见的激活函数用泰勒展开：

![Image](https://github.com/user-attachments/assets/b5d7ac35-c3de-47d5-9b49-3b12d03c9ef5)

可以看到tanh和relu激活函数在0点附近时，是基本满足上述要求的，但是sigmoid激活函数不满足条件，需要对其进行修改。

